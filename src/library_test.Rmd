---
title: "Lickometer library"
output:
  html_document:
    df_print: paged
---

# libs

```{r}
pacman::p_load(
         tidyr,
	       tidyverse,
	       dplyr,
	       purrr,
	       readr,
	       lubridate,
	       validate,
	       pointblank,
         furrr
	       )
```

# [1.0] load_data

Data is loaded from single folder containing all relevant files

```{r}
# Main function
load_data <- function(path){
	load_data_as_char(path) %>% 
  format_data_type(.)
}

# loading data as char allows us to validate data against regex pattern
load_data_as_char <- function(path){
  # Read files
	ds <- list.files(
		   path = path,
		   pattern = "_[0-9]+\\.csv",
		   full.names = T
		   ) %>%
	set_names() %>%
	purrr::map_dfr(
			  read_csv, .id = "source", col_types = cols(.default = "c")
			  )
  
  # Issue information for files loaded
  warning(paste("The following data files were loaded:", 
                paste(unique(ds$source), collapse = "\n"), 
                sep = "\n"))
	
	# Return dataset
	return(ds)
}

# correct data type for all relevant variables
format_data_type <- function(raw_file){
			raw_file %>%
				mutate(
					ID = as.factor(ID),
					sensor = as.factor(sensor),
					tiempo = as.numeric(tiempo),
					actividad = as.numeric(actividad),
					evento = as.numeric(evento),
					exito = as.factor(exito),
				  fecha = source_to_date(source),
				  fecha_ms = source_to_date_posix_ms(source),
				  tiempo_fecha_ms = fecha_ms + tiempo,
				  tiempo_fecha = source_to_date_sec(source) + lubridate::seconds(tiempo / 1000)
	       )
}

# source is the complete path with the filename that contains the date of
# data collection, this function extracts such date and transforms it 
# into the proper format
source_to_date <- function(x) {
	sub('\\..*$', '', basename(x)) %>%
		lubridate::ymd_hms(.) -> tmp
	out <- lubridate::ymd(
			      paste(
				    lubridate::year(tmp),
				    lubridate::month(tmp),
				    lubridate::day(tmp)
				    )
			      )
	return(out)
}

# same as source_to_date but keeps the whole format (with seconds)
source_to_date_sec <- function(x) {
	sub('\\..*$', '', basename(x)) %>%
		lubridate::ymd_hms(.) -> out
	return(out)
}

# sames as source_to_date but in posix epoch
source_to_date_posix_ms <- function(x) {
	sub('\\..*$', '', basename(x)) %>%
		lubridate::ymd_hms(.) %>%
		lubridate::seconds(.) %>%
		as.numeric() * 1e3
}
```

# [1.1] test load data

```{r}
path <- "../test/files"
test_data <- load_data(path)
```

# [2.0] load_metadata

```{r}
# mutate metadata into the proper format
# if coerce fails "NA" comes out
load_metadata <- function(x) {
  # x is the path to a csv files that contains all metadata for the experiment. 
  # Check : that column names are correct and with the correct data type.
  # correct column names
  col_names <- c(
			       "n_sesion",
			       "fecha",
			       "droga",
			       "dosis",
			       "ID",
			       "hora_inicio",
			       "hora_termino",
			       "n_licometro",
			       "estimulo_spout_1",
			       "estimulo_spout_2",
			       "licks_inicio_spout_1",
			       "licks_inicio_spout_2",
			       "licks_fin_spout_1",
			       "licks_fin_spout_2",
			       "eventos_inicio_spout_1",
			       "eventos_inicio_spout_2",
			       "eventos_fin_spout_1",
			       "eventos_fin_spout_2",
			       "ml_consumidos_spout_1",
			       "ml_consumidos_spout_2"
			       )
  	# correct regex patterns
		patterns <- c(
			      "[0-9]", # n_sesion
			      "[0-9]{4} [0-9]{2} [0-9]{2}", # fecha
			      "[a-z]+_[a-z]+_[a-z]+_[a-z]+", # droga droga1_droga2_metodo_area
			      "[0-9]+_[0-9]+_[a-z]+", # dosis dosis1_dosis2_unidad de medida
			      "[0-9]", # ID
			      "[0-9]{2} [0-9]{2} [0-9]{2}", # hora_inicio
			      "[0-9]{2} [0-9]{2} [0-9]{2}", # hora_termino
			      "[0-9]", # n_licometro
			      "[a-z]", # estimulo_spout_1
			      "[a-z]", # estimulo_spout_2
			      "[0-9]+", # licks_inicio_spout_1
			      "[0-9]+", # licks_inicio_spout_2
			      "[0-9]+", # licks_fin_spout_1
			      "[0-9]+", # licks_fin_spout_2
			      "[0-9]+", # eventos_inicio_spout_1
			      "[0-9]+", # eventos_inicio_spout_2
			      "[0-9]+", # eventos_fin_spout_1
			      "[0-9]+", # eventos_fin_spout_2
			      "[0-9]+", # ml_consumidos_spout_1
			      "[0-9]+" # ml_consumidos_spout_2
		)
  
	# run check for names and values
	colCheck <- sapply(1:length(col_names), function(colNameIndex) {
	  colName <- col_names[colNameIndex]
    index <- which(names(x) == colName) # which column in metadata is colName	  
	  if(length(index) == 1) { # TRUE if colName exists only once
	    #  TRUE if column, except NA matches pattern
	    ifelse(all(na.omit(str_detect(x %>% pull(colName), patterns[colNameIndex]))), TRUE, FALSE) 
	      }
	  })
	  
  # Create message with results of name and format check
	if(all(colCheck)) {
	  message("Import metada ok!!")
	} else {
	  stop(paste("WARNING: The following required columns are either not found on the dataset or have the incorrect format:",
	                paste(col_names[colCheck], collapse = "\n"), 
	                sep =  "\n"))
	  }
	  
	# Process metadata. Select desired columns and format
	out <- x %>% 
	  select(all_of(col_names)) %>% 
	  filter(if_any(everything(), ~ !is.na(.))) %>% # Filter out all NA rows
	  mutate(
		       n_sesion = as.factor(n_sesion),
		       fecha = lubridate::ymd(fecha),
		       droga = as.character(droga),
		       dosis = as.character(dosis),
		       ID = as.factor(ID),
		       hora_inicio = lubridate::hms(hora_inicio),
		       hora_termino = lubridate::hms(hora_termino),
		       n_licometro = as.factor(n_licometro),
		       estimulo_spout_1 = as.character(estimulo_spout_1),
		       estimulo_spout_2 = as.character(estimulo_spout_2),
		       licks_inicio_spout_1 = as.numeric(licks_inicio_spout_1),
		       licks_inicio_spout_2 = as.numeric(licks_inicio_spout_2),
		       licks_fin_spout_1 = as.numeric(licks_fin_spout_1),
		       licks_fin_spout_2 = as.numeric(licks_fin_spout_2),
		       eventos_inicio_spout_1 = as.numeric(eventos_inicio_spout_1),
		       eventos_inicio_spout_2 = as.numeric(eventos_inicio_spout_2),
		       eventos_fin_spout_1 = as.numeric(eventos_fin_spout_1),
		       eventos_fin_spout_2 = as.numeric(eventos_fin_spout_2),
		       ml_consumidos_spout_1 = as.numeric(ml_consumidos_spout_1),
		       ml_consumidos_spout_2 = as.numeric(ml_consumidos_spout_2)
		       )
	
	# Check number of NAÂ´s per column
	countNA <- out %>% 
	  map(., ~sum(is.na(.))) %>% 
	  as_tibble %>% 
	  select_if(~ .x != 0) 
	if (any(countNA != 0)) {
  	message(paste("WARNING: The following columns of the metadata contain one or more NA values.",
	              paste(names(countNA), collapse = "\n"),
	              sep = "\n")) 
	  }
	 
  return(out) # return data
	}
	
		
```

# [2.1] test load_metadata

```{r}
metadata <- load_metadata(
				  read_csv("../test/files/metadata_fed_mayo.csv", col_types = cols(.default = "c"))
				  )
```
# [3.0] load_experiment

This function is a wrapper for load_data and load_metadata + the merge and selects valid data based on start and end times. This creates the concept of an experiment is the combination of the data files and the metadata

```{r}

load_experiment <- function(metadataFileName, data_directory_path) {
  # Load meta-data, importing as character allows for regex pattern matching
  if (file.exists(metadataFileName)) {
	  metadata <- load_metadata(read_csv(metadataFileName, col_types = cols(.default = "c")))  
  } else {
    stop("The specified metadata file does not exist.")
      }

  # Check that meta-data contains a combination of one mice per day
  check <- metadata %>% 
    count(ID, fecha) %>% 
    filter(n != 1) %>% 
    select(ID, fecha) %>% 
    mutate(info = paste(ID, fecha, sep = "|")) %>% 
    pull(info)
  if(length(check) != 0) {
        stop(paste("The following combinations of ID  and date are repeated", 
                   paste(check, collapse = "\n"),
                   sep = "\n"))
    }
  
  # Load lickometer data
	data_file <- load_data(data_directory_path)
	# IOW labs lickometer data should NOT produce any NA. 
	# an NA is likely data corruption and needs to be informed
	countNA <- data_file %>% 
	  filter(if_any(ID:tiempo_fecha, ~is.na(.x)))
	if (nrow(countNA) != 0) {
  	message(paste("WARNING: The following lickometer data files contain one or more NA values. This might affect analysis results",
	              paste(names(countNA), collapse = "\n"),
	              sep = "\n")) 
	  }	
	
	# Merge data
	out <- data_file %>%
	  # ID and fecha should be enough, as long as animals only
	  # get 1 experimental session per day
		left_join(metadata, by = c("ID", "fecha"))	%>% 
		# at this point all data should be in the correct format
		# so we add date in posix_ms for time activity correction
		# function
		mutate(
		  hora_inicio_ms = paste(fecha, hora_inicio, sep = " ") %>%
		    lubridate::ymd_hms() %>%
		    lubridate::seconds() %>%
					as.numeric() * 1e3,
		  hora_fin_ms = paste(fecha, hora_termino, sep = " ") %>%
		    lubridate::ymd_hms() %>%
		    lubridate::seconds() %>%
		    as.numeric() * 1e3,
		  tipo_recompensa = if_else(sensor == 0, estimulo_spout_1, estimulo_spout_2))
		
	  # Check number of rows of data files that have meta_data
	  if(nrow(out) != nrow(data_file)) {
	    stop("The dimensions of the merged experiment data and metadata are different from those of the experiment data alone. Please inspect your metadata carefully")
	  } 
	
	# Filter valid data for each mice based on the timestamp of licks that is within the start and end time of the session as obtained from the metadata.
out <- out %>% 
  filter(
    hora_inicio_ms <= tiempo_fecha_ms &
    tiempo_fecha_ms <= hora_fin_ms) %>% 
  mutate(timestamp = tiempo_fecha_ms - hora_inicio_ms) %>%  # This corrects time based on the metadata start of the session
return(out)
}
```

# [3.1] test merge_inputs

```{r}
test_data <- load_experiment(
	     "../test/files/metadata_fed_mayo.csv",
	     "../test/files"
	     )
```
# [3.2] minimal plot

non-corrected data

```{r}
test_data %>% 
  ggplot(aes(
    timestamp,
    actividad,
    color = tipo_recompensa
  )) +
  geom_line() +
  facet_grid(n_sesion~ID)
```

# [4.0] Uncumulate data

This function will convert the licks and events to non-cumulative counts

```{r}
# uncumulate
uncumulate <- function(merged_input){
         merged_input %>%
                 group_by(ID, n_sesion, sensor) %>%
                 arrange(timestamp, .by_group = TRUE) %>% # This is to make sure data is ordered by time_stamp
                 mutate(
                   # NA_real_ is to preserve dbl class. The use of a nested if_else is necessary so the first activity
                   # log doesn't get assigned a NA value. Because events starts at zero, the correction does not apply
                        evento_no_cum = if_else(evento - lag(evento) > 0, 1, NA_real_),
                        actividad_no_cum = if_else(actividad == min(actividad), 1, 
                                                   if_else(actividad - lag(actividad) > 0, 1, NA_real_))
                        ) %>%
                 ungroup()
}
```

# [5.1] test uncumulate

```{r}
uncumulate_data <- test_data %>% 
  uncumulate()

uncumulate_data %>% 
  filter(
    ID == "320",
    n_sesion == "1",
    tipo_recompensa == "sacarosa"
  ) %>% 
  select(
    evento,
    evento_no_cum
  )
```

# [6.0] interval_estimate

This is the same as inter lick intervals

```{r}
# interval estimate
# This function creates a new vector that calculates the interval for a given time-stamp vector
interval_estimate <- function(x){ x-lag(x) }

```

# [6.1] test interval_estimate

```{r}
# Plot frequency of licks
test_data %>% 
  uncumulate() %>% 
  group_by(ID, n_sesion, sensor) %>% 
  mutate(actividad.interval = interval_estimate(timestamp)) %>% 
  ggplot(aes(x = actividad.interval)) +
  facet_wrap(~ tipo_recompensa, scales = "free") +
  geom_histogram()

# Plot frequency of events
test_data %>% 
  uncumulate() %>% 
  group_by(ID, n_sesion, sensor) %>% 
  filter(evento_no_cum == 1) %>% 
  mutate(evento.interval = interval_estimate(timestamp)) %>% 
  ggplot(aes(x = evento.interval)) +
  facet_wrap(~ tipo_recompensa, scales = "free") +
  geom_histogram()
```
# [7.0] bin_calculation

```{r}
bin_calculation <- function(merged_data, session_length_min, bin_size_min){
  
  # Check that no data for the corrected time stamp is above the session length
  countCheck <- merged_data %>% 
    mutate(flag = timestamp > session_length_min * 60 * 1e3) %>% 
    filter(flag) %>% # select only the data that is above the  
    count(ID, fecha, flag) %>% 
    select(!flag) %>% 
    unite("msg", everything(), sep = "\t") %>% 
    pull(msg)
  
  # Display message
  message(paste("WARNING: The following mice have timestamps outside the specificed session length",
                "MICE\tDATE\tTIMESTAMPS",
                paste(countCheck, collapse = "\n"),
                sep = "\n"))
  
  # Calculate bins
  merged_data %>%
		group_by(
			 ID, n_sesion) %>%
		mutate(
		  # variable timestamp contains time relative to the start of the session, bins are calculated based on the 
		  # length of the session
		      bins = cut(
		        timestamp,
		        breaks = seq(0, session_length_min * 60 * 1e3, by = bin_size_min * 60 * 1e3),
		        labels = FALSE,
		        include.lowest = TRUE))
}
```

# [7.1] test bin_calculation

```{r}
bin_calculation_data <- test_data %>% 
  uncumulate() %>% 
  bin_calculation(., 60, 10) # 1 h session, 10 min bins

bin_calculation_data %>% 
  filter(
    tipo_recompensa == "sacarosa"
    ) %>% 
  group_by(
    ID,
    n_sesion,
    bins
  ) %>% 
  summarise(
    licks_per_bin = n()
  ) %>% 
  ggplot(aes(
    bins,
    licks_per_bin
  )) +
  geom_point() +
  geom_line() +
  facet_grid(n_sesion ~ ID)
```

# [8.0] burst_estimates

burst_estimates calculates clusters of licks based on a fixed threshold
typical = 250 - 1000 ms

```{r}
burst_analysis <- function(dataset, threshold){ 
  # Checks for presence of interval in dataset
  if(sum(names(dataset) == "interval") != 1) {
    stop("function burst_analysis requires one column named interval that contains intervals to calculate bursts")
  }
  
   # Checks for presence of interval in dataset
  if(sum(names(dataset) == "timestamp") != 1) {
    stop("function burst_analysis requires one column named timestamp that contains intervals to calculate bursts")
  }
  
  # Run function
	burst_ds <- dataset %>%
		group_by(ID, sensor, n_sesion) %>%
	  # this determines whether each timestamp belongs in a cluster or not
		mutate(
		  cluster_bool = ifelse(interval > threshold, "out_cluster", "in_cluster")
		  ) 
	
	# Get burst summaries stats
	burst_summary <- burst_ds %>%
 	 group_by(ID, sensor, n_sesion, .add = TRUE) %>% 
	 group_split() 
	
	burst_summary <- lapply(burst_summary, function(ds) {
	  # Fix first assignment
	  ds$cluster_bool[1] <- "in_cluster" # First timestamp will always be its own cluster
	  
	  # Get estimates of burst by using rle R's base function 
	  s <- ds %>% 
	    group_by(ID, sensor, n_sesion, tipo_recompensa) %>% 
	    summarise(
	          values = rle(cluster_bool)$values, # inside our outside burst
	          length = rle(cluster_bool)$lengths) # number of timestamps asssociated with the burst
	   
	 # Create temporary index variable   
	 index <- cumsum(s$length) 
	 index2 <- index - (s$length - 1)
	 s$burst_timestamp_start <- ds$timestamp[index2] # get timestamp for start of cluster	 
	 s$burst_timestamp_end <- ds$timestamp[index] # get timestamp for end of cluster
	 return(s)
	}) %>% 
	bind_rows() %>% 
	rename(cluster_length = length)  

	# Return data
	return(burst_summary)
	}
```

# [8.1] test burst_summary_data

```{r}
burst_data <- test_data %>% 
  uncumulate() %>% 
  group_by(ID, n_sesion, sensor) %>% 
  mutate(interval = interval_estimate(timestamp)) %>% 
  burst_analysis(., 500)

burst_data %>% 
  filter(
    tipo_recompensa == "sacarosa",
    ID == "320",
    n_sesion == "1"
  ) %>% 
  mutate(yval = ifelse(values == "in_cluster", 1, 0)) %>% 
  ggplot(aes(
    x = burst_timestamp_start,
    y = yval
  )) + 
  geom_step()
```

# [9.0] synch_lick_event

```{r}
synch_lick_event <- function(ds, parallel){
  require(data.table)
  
  # Check parallel option is boolean
  if(class(parallel) != "logical") {
    stop("Parallel option has to be logical")
  }
  
  # Check that parallel option argument was provided
  if(parallel) {
    # Print message
    message("Parallel option uses the parallel package")
    require(pbmcapply)
    require(parallel)
    }
  
# Extract row_index. This is necessary becaus  
  ds <- ds %>%
	mutate(rn = row_number())
row_index <- ds %>% select(rn)

# Iterate over group
out <- ds %>%
	group_split(
		    ID,
		    n_sesion,
		    sensor
		    ) %>%
	map_dfr(., function(x){
	  # first we get row indices of events
		event_indices <- 	x %>%
				filter(evento_no_cum == 1) %>%
				pull(rn)
		      
	  # this tibble defines for every event
		# the previous and the next event
	   tibble(
			      start_ = lag(unlist(event_indices), default = min(row_index)),
			      end_ = lead(unlist(event_indices), default = max(row_index)),
			      idx = event_indices,
			      sensor_event = unique(x$sensor)
			      )
})
	
# Run sequence with and without parallel processing
 index_list <- split(out, seq(nrow(out))) 
 
 if (parallel) {
   P <- pbmcapply(index_list, function(x) {
    		rows <- x$start_[1]:x$end_[1] # This is a list of positions within 
				ds[rows,] %>%
					mutate(
					       event_id = x$idx[1],
					       sensor_event = x$sensor_event[1],
					       tiempo_peri_evento = timestamp - timestamp[which(rn == x$idx[1])]
					) %>% 
				  filter(sensor == sensor_event)
				}, mc.cores = 3) %>%
     data.table::rbindlist() %>% 
     mutate(event_id = as.numeric(as.factor(event_id)))
 #  bind_rows() %>% 
   #filter(sensor = sensor_event) 
 } else {
    require(pbapply) # package required for progress bar
   P <- pblapply(index_list, function(x) {
    		rows <- x$start_[1]:x$end_[1] # This is a list of positions within 
				ds[rows,] %>%
					mutate(
					       event_id = x$idx[1],
					       sensor_event = x$sensor_event[1],
					       tiempo_peri_evento = timestamp - timestamp[which(rn == x$idx[1])]
					) %>% 
				  filter(sensor == sensor_event)
				}) %>% 
     data.table::rbindlist() %>% 
     mutate(event_id = as.numeric(as.factor(event_id)))
   #  bind_rows() %>% # This function is extremely slow
    # filter(sensor == sensor_event)   
   }
 return(P)   
}
```

# [11.1] test synch_lick_event

```{r}
synch_lick_event_data <- test_data %>% 
  uncumulate()  %>% 
  synch_lick_event(., parallel = FALSE)
```

